{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets as datasets\n",
    "import torchvision.transforms as transform\n",
    "t = transform.Compose([transform.ToTensor(),transform.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_data = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=t)\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=t)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining fulling connected layer \n",
    "class FClayer:\n",
    "        def __init__(self,Sin,Sout):\n",
    "            self.Sin = Sin\n",
    "            self.Sout = Sout\n",
    "            self.W = np.random.randn(Sin,Sout)/np.sqrt(Sin)    # initializing weights and bias matrices with random normal distribition \n",
    "            self.b = np.random.randn(Sout,1)/np.sqrt(Sin)\n",
    "            self.inV = None\n",
    "\n",
    "        def forward(self,inV):\n",
    "            outV = np.matmul(np.transpose(self.W),inV)+ self.b\n",
    "            self.inV = inV  #storing the input vector for backpropagation\n",
    "            return outV\n",
    "\n",
    "        def backprop(self,grad):\n",
    "            Lgrad = np.matmul(self.W,grad)  #multiply the input grad  with the gradient wrt input which is the weight matrix \n",
    "            self.W = self.W - 0.005*np.matmul(self.inV,np.transpose(grad)) #updating the weights and biases with their respective gradients \n",
    "            self.b = self.b - 0.005*grad            \n",
    "            return Lgrad\n",
    "\n",
    "class Relu:\n",
    "        def __init__(self):\n",
    "            self.inV = None\n",
    "\n",
    "        def forward(self,inV):\n",
    "            outV = np.zeros((len(inV),1))\n",
    "            for i in range(0,len(outV)):\n",
    "                outV[i][0] = max(0,inV[i][0])\n",
    "            self.inV =inV\n",
    "            return outV\n",
    "\n",
    "        def backprop(self,grad):\n",
    "            grad[self.inV<0]= 0;\n",
    "            return grad\n",
    "\n",
    "class Softmax:\n",
    "        def __init__(self):\n",
    "            self.outV = None\n",
    "\n",
    "        def forward(self,inV):\n",
    "            outV = np.exp(inV)\n",
    "            outV = outV/np.sum(outV)\n",
    "            self.outV = outV\n",
    "            return outV\n",
    "\n",
    "        def backprop(self,grad):\n",
    "            size = self.outV.shape[0]\n",
    "            Lgrad = np.zeros((size,size))\n",
    "            for i in range(size):\n",
    "                for j in range(size):\n",
    "                    if(i==j):\n",
    "                        Lgrad[i][i] = self.outV[i][0]*(1-self.outV[i][0]) \n",
    "                    else:\n",
    "                        Lgrad[i][j] = -self.outV[i][0]*self.outV[j][0]\n",
    "                        Lgrad[j][i] = -self.outV[i][0]*self.outV[j][0]\n",
    "            grad = np.matmul(Lgrad,grad)\n",
    "            return grad\n",
    "class CEloss:\n",
    "        def __init__(self):\n",
    "            self.label = None\n",
    "            self.inV = None\n",
    "\n",
    "        def forward(self,inV,label):\n",
    "            Loss = -np.log(inV[label][0])\n",
    "            self.label = label\n",
    "            self.inV = inV\n",
    "            return Loss\n",
    "\n",
    "        def backprop(self,grad):\n",
    "            grad = np.zeros((self.inV.shape[0],1))\n",
    "            grad[self.label][0] = -1/self.inV[self.label][0]\n",
    "            return grad\n",
    "\n",
    "class neuralnet:\n",
    "        def __init__(self):\n",
    "            self.Layers = np.array([])\n",
    "\n",
    "        def forward(self,inV,label):\n",
    "            outV = inV\n",
    "            for i in range(0,len(self.Layers)-1):\n",
    "                outV = self.Layers[i].forward(outV)\n",
    "            outV = self.Layers[len(self.Layers)-1].forward(outV,label)\n",
    "            return outV\n",
    "\n",
    "        def backprop(self):\n",
    "            grad = np.array([])\n",
    "            for i in range(len(self.Layers)-1,-1,-1):\n",
    "                grad = self.Layers[i].backprop(grad)\n",
    "               # print(grad.shape)\n",
    "            return\n",
    "\n",
    "        def addlayer(self,layer):\n",
    "            self.Layers = np.append(self.Layers,layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = neuralnet()\n",
    "NN.addlayer(FClayer(3072,400))\n",
    "NN.addlayer(Relu())\n",
    "NN.addlayer(FClayer(400,50))\n",
    "NN.addlayer(Relu())\n",
    "NN.addlayer(FClayer(50,2))\n",
    "NN.addlayer(Softmax())\n",
    "NN.addlayer(CEloss())\n",
    "\n",
    "test_count = 0\n",
    "train_count = 0\n",
    "test_loss = 0\n",
    "train_loss = 0\n",
    "lossarray = np.array([])\n",
    "testlossarray = np.array([])\n",
    "\n",
    "for epoch in range(5):\n",
    "    for image,label in trainloader:\n",
    "        if label<2:\n",
    "            image = (image.view(-1,1)).numpy()\n",
    "\n",
    "            train_loss += NN.forward(image,label)\n",
    "            NN.backprop()\n",
    "\n",
    "            train_count += 1\n",
    "\n",
    "            if train_count%1000==0:\n",
    "                correct = 0\n",
    "                test_count = 0\n",
    "\n",
    "                for image, label in testloader:\n",
    "                    if label<2:\n",
    "                        image = (image.view(-1,1)).numpy()\n",
    "\n",
    "                        test_loss += NN.forward(image,label)\n",
    "\n",
    "                        prob = NN.Layers[5].outV\n",
    "                        top_class = np.argmax(prob)\n",
    "                        if label == top_class:\n",
    "                            correct += 1\n",
    "\n",
    "                        test_count += 1\n",
    "\n",
    "                print(f\"Epoch {int(train_count/500)}..\" f\"Train_loss: {train_loss/500:.3f}..\" f\"Test_loss: {test_loss/test_count:.3f}..\"\n",
    "                      f\"Accuracy: {correct/test_count:.3f}..\")\n",
    "\n",
    "                lossarray = np.append(lossarray,train_loss/500)\n",
    "                testlossarray = np.append(testlossarray,test_loss/test_count)\n",
    "                test_loss = 0\n",
    "                train_loss = 0\n",
    "\n",
    "plt.plot(lossarray, label='Training loss')\n",
    "plt.plot(testlossarray, label='Testing loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('tr_2.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
